\documentclass[15pt]{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2018}
     

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[english]{babel}
\usepackage{hanging} % for hanging references
\usepackage{physics} % for bra ket notation
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{wrapfig}
\graphicspath{ {./images/} }

\title{Project 3 Report}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Yizhan Ao\\
  Email: josephao@umd.edu   \\
  UID: 116022064\\
  \And
  Yingqiao Gou\\
  Email: ygou@terpmail.umd.edu\\
  UID: \\
}

\begin{document}
 
\maketitle

\begin{abstract}
You will be graded primarily based on your report.
The most important part is to understand and explain why your algorithm is working or not working on a given dataset. What do you think are the drawbacks of your code!?

We want you to demonstrate an understanding of the concepts involved in the project, and to show the output produced by your code.
Include visualizations of the output of each stage in your pipeline (as shown in the system diagram on page 2), and a description of what you did for each step. Assume that we’re familiar with the project, so you don’t need to spend time repeating what’s already in the course notes. Instead, focus on any interesting problems you encountered and/or solutions you implemented.
\end{abstract}

\section{Concept}
The aim of this project is to segment deformable object from a given video sequence. This document just provides an overview of what you need to do. For a full breakdown of how each step in the pipeline works, see the course notes for this project.

\section{Setting up local windows}
Finding and saving the window's midpoints was all that was required to initialize the local windows. These mid-points were just points uniformly dispersed over the first mask's shape. InitLocalWindows.m has previously done this for us. We didn't alter anything except the problem that was in the code at the time.
\section{Initializes color models}
We used rgb2lab to convert the original frame from RGB to LAB colorspace in order to initialize the color models. Then, using bwdist, to calculate the distance matrix of the mask outline in each window. This provides me the distance to the original mask's edge, which allows us to eliminate points that are too near to the edge from GMM training since color changes dramatically at this edge and may not be representational of either the foreground or background.
\begin{equation}
p_{c}(x)=p_{c}(x \mid \mathcal{F}) /\left(p_{c}(x \mid \mathcal{F})+p_{c}(x \mid \mathcal{B})\right)
\end{equation}
The color model is started by estimating a Gaussian Mixture Model for each window's foreground pixels, and then computing a GMM for the background pixels (where the mask is produced by the user's roipoly result). Because there is little change in the colors of the foreground and background in the instance of the turtle, we utilized GMM of size 1; the number k varied for the other sets of photos.
\section{Initializes color confidences}
The color confidence is derived using the Rotobrush paper's algorithm and is dependent on the color model. Every window's color confidences are saved. [0,1] is a nice method to represent how effectively the foreground and background are separated.
\begin{equation}
f_{c}=1-\frac{\int_{W_{k}}\left|L^{t}(x)-p_{c}(x)\right| \cdot \omega_{c}(x) d x}{\int_{W_{k}} \omega_{c}(x) d x}
\end{equation}

\section{Calculates local window movement based on optical flow between frames}

\section{Finds affine transform between two frames.}
\section{Update shape and color models.}

\section{Code + Algorithm}

When running MyRotobrush.m, it must load a set of frames from the folder Frames/. Assume that the frames will be jpeg images of the form 1.jpg, 2.jpg, etc. Your program must then prompt the user to specify a region of interest in the first frame (for example using the roipoly tool), and then track the specified object through the rest of the frames. For each frame, draw the tracked boundary in red and save the result with the same filename in Output/.
\\
When run, MyRotobrush.m must load a set of images from Images/Input/, and return the resulting panorama.
\\
Functions Allowed\\
For this project, \\
roipoly, fitgmdist, estimateGeometricTransform, opticalFlowFarneback, vl\_sift\\ (http://www.vlfeat.org/overview/sift.html), \\
imfilter, conv2, imrotate, im2double, rgb2gray, fspecial, imtransform, imwarp (and imref2d),\\ meshgrid, sub2ind, ind2sub and all other plotting and matrix operation/manipulation functions are allowed.


myRotobrush.m: Wrapper function.\\
initLocalWindows.m: Creates local windows on boundary of mask.\\
initColorModels.m: Initializes color models.\\
initShapeConfidences.m: Initializes shape confidences.\\
localFlowWarp.m: Calculates local window movement based on optical flow between frames.\\
calculateGlobalAffine.m: Finds affine transform between two frames.\\
updateModels.m: Update shape and color models.\\
showLocalWindows.m: Plots local windows.\\
showColorConfidences.m: Plots the color confidence for each local window.\\
equidistantPointsOnPerimeter.m: Find equally spaced points along the perimeter of a polygon
\\~\\

Setup Local Windows: 5 pts\\
Initialize Color Models: 10pts\\
Compute Color Model Confidence: 5 pts\\
Initialize Shape Model: 10 pts\\
Compute Shape confidence: 5 pts\\
Estimate Entire-Object Motion: 5 pts\\
Estimate Local Boundary Deformation: 10 pts\\
Update Color Model (and color confidence): 15 pts\\
Combine Shape and Color Models: 5 pts\\
Merge Local Windows: 10 pts\\
Extract final foreground mask: 20 pt\\


\section{Output}
You have been provided the frames from five video clips. Run your code on each set of frames and create videos, named as indicated above. For each video track the following:
\\
Frames1: Track the turtle. Source: An underwater video captured by Chahat at Lakshadweep islands, India.\\
Frames2: Track the motorcycle and rider. Source.\\
Frames3: Track the gymnast. Source.\\
Frames4: Track the powerlifter, without the weights. Source.\\
Frames5: Track the lizard (including tail). Source.\\

\section{Conclusion}
You have been provided the frames from five video clips. Run your code on each set of frames and create videos, named as indicated above. For each video track the following:\\

Frames1: Track the turtle. Source: An underwater video\\ captured by Chahat at Lakshadweep islands, India.\\
Frames2: Track the motorcycle and rider. Source.\\
Frames3: Track the gymnast. Source.\\
Frames4: Track the powerlifter, without the weights. Source.\\
Frames5: Track the lizard (including tail). Source. \\

% \begin{figure}[H]
% \centering
% \begin{subfigure}{.5\textwidth}
%   \includegraphics[height=30mm\linewidth,left]{images/tree tensor.png}
%   \caption{QNN with Tree Tensor structure with 4 qubits [7]}
%   \label{fig:sub1}
% \end{subfigure}%
% \begin{subfigure}{.5\textwidth}
%   \includegraphics[height=30mm\linewidth,right]{images/step controlled.png}
%   \caption{QNN with Step Controlled structure with 2 step-control connections [7]}
%   \label{fig:sub2}
% \end{subfigure}
% % \caption{A figure with two subfigures}
% \label{fig:test}
% \end{figure}


\section*{References}
\small
\begin{hangparas}{.25in}{1}
[1] Nathan Wiebe, Ashish Kapoor, Krysta M. Score (2015). Quantum Deep Learning. Microsoft Research, Redmond, WA. arXiv:1412.3489 [quant-ph].


\end{hangparas}

\end{document}
